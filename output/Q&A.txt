Optimization terminated successfully.
         Current function value: 0.240999
         Iterations 8
result.coef:                             Logit Regression Results                           
==============================================================================
Dep. Variable:                      y   No. Observations:                  176
Model:                          Logit   Df Residuals:                      157
Method:                           MLE   Df Model:                           18
Date:                Fri, 09 Nov 2018   Pseudo R-squ.:                  0.6522
Time:                        14:02:51   Log-Likelihood:                -42.416
converged:                       True   LL-Null:                       -121.95
                                        LLR p-value:                 1.269e-24
======================================================================================
                         coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
Hb_min                -0.3721      0.323     -1.153      0.249      -1.005       0.261
MCHC_max              -1.3244      0.641     -2.068      0.039      -2.580      -0.069
MCHC_min               0.4982      0.571      0.873      0.383      -0.620       1.617
age                    1.4203      0.479      2.962      0.003       0.481       2.360
BUN_max                3.5894      1.877      1.912      0.056      -0.089       7.268
BUN_min               -1.5787      1.731     -0.912      0.362      -4.971       1.814
HbS_max                0.6303      0.501      1.257      0.209      -0.352       1.613
HbS_min               -0.8185      0.626     -1.307      0.191      -2.046       0.409
PTT_min                0.0463      0.283      0.164      0.870      -0.508       0.601
PTT_max                0.6031      0.271      2.226      0.026       0.072       1.134
K_max                  0.0012      0.465      0.003      0.998      -0.909       0.912
Glc_max                0.2293      0.368      0.623      0.533      -0.492       0.950
adlocation_CR/PM      -0.4921      0.726     -0.678      0.498      -1.916       0.931
adlocation_ERA         0.9760      0.860      1.134      0.257      -0.710       2.662
insurance_Medicare    -0.4501      0.547     -0.823      0.410      -1.522       0.622
insurance_Private     -0.3623      0.454     -0.798      0.425      -1.252       0.527
religion_JEWISH       -0.2291      0.568     -0.404      0.687      -1.342       0.883
mastatus_WIDOWED       0.3868      0.412      0.939      0.347      -0.420       1.194
language_ENGL         -0.6539      0.333     -1.967      0.049      -1.306      -0.002
======================================================================================

1. 通常没有什么模型是通吃所有类型的数据集，只有最合适的；我们的数据集有几个特征，缺失值多/特征稀疏/数据量小/样本正负不均衡/因子变量与数值型变量都有，结果表明随机森林和逻辑回归表现都很好，差异也很小，但都明显比决策树好，有几个原因：逻辑回归对于小样本，特别是与目标特征有线性关系的特征颇多的场景（数据线性可分），运算速度极快也不容易过拟合，相应的，随机森林通过降低方差的方式提升稳定性与泛化能力，也不易过拟合，但如果样本量太小，可能就会过拟合，所以分类器的好坏和数据关系很大；而树典型特征就是高方差，任何节点任意的偏差都会导致结果的改变因而非常不稳定，极易过拟合。
2. 为了充分发挥所有模型的最大功效，我们将变量选择的过程与分类结果紧密相连，对于逻辑回归，为了避免过拟合，我们另外还添加了l2正则项，拟合过程中让正则权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。对于随机森林，利用自主采样，通过约束树的深度与叶子生长的广度，控制树的生长提升稳定性避免过拟合。
3. 逻辑回归和随机森林的feature importance不一样是正常的，因为他们本来评价指标就是不同。逻辑回归是通过系数关系找出与目标特征线性关系最强的特征，而随机森林则是通过gini系数找出特征分割组合对整棵树影响最大的特征。